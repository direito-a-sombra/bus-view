{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be788b42",
   "metadata": {},
   "source": [
    "# Train DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3889e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning\n",
    "\n",
    "!wget -q https://github.com/direito-a-sombra/bus-view/releases/latest/download/imgs.tar.gz\n",
    "!wget -q https://raw.githubusercontent.com/direito-a-sombra/bus-view/refs/heads/main/data/training/train_boxes.json\n",
    "!wget -q https://raw.githubusercontent.com/direito-a-sombra/bus-view/refs/heads/main/data/training/train_files.json\n",
    "!tar -xzf imgs.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afcbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from PIL import Image as PImage\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoImageProcessor, DetrForObjectDetection\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import stack as t_stack\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573071e0",
   "metadata": {},
   "source": [
    "## Create COCO-ish Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3892e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info about all training image files\n",
    "\n",
    "IMG_DIR = \"./imgs\"\n",
    "TRAINING_FILES_JSON = \"./data/training/train_files.json\"\n",
    "\n",
    "with open(TRAINING_FILES_JSON, \"r\") as ifp:\n",
    "  training_files = json.load(ifp)\n",
    "\n",
    "id2path = {}\n",
    "for label, d2fs in training_files.items():\n",
    "  for d ,fs in d2fs.items():\n",
    "    for f in fs:\n",
    "      id2path[f\"{d}/{f}\"] = { \"dir\": d, \"name\": f }\n",
    "\n",
    "id2idx = { id:idx for idx,id in enumerate(sorted(id2path.keys())) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47dde37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep COCO-ish dataset\n",
    "\n",
    "LABEL2ID = {\n",
    "  \"bus_stop\": 0,\n",
    "  \"bus_sign\": 1,\n",
    "}\n",
    "ID2LABEL = { id:label for label,id in LABEL2ID.items() }\n",
    "\n",
    "with open(\"./data/training/train_boxes.json\", \"r\") as ifp:\n",
    "  box_info = json.load(ifp)\n",
    "\n",
    "tids, vids = train_test_split(list(box_info.keys()), test_size=0.2, random_state=1010)\n",
    "\n",
    "tids = set(tids)\n",
    "vids = set(vids)\n",
    "\n",
    "annotated = {}\n",
    "\n",
    "for img_id, objs in box_info.items():\n",
    "  split = \"train\" if img_id in tids else \"val\"\n",
    "  img_dir = id2path[img_id][\"dir\"]\n",
    "  img_name = id2path[img_id][\"name\"]\n",
    "\n",
    "  img_annotations = []\n",
    "  for label, (x0,y0,x1,y1) in objs.items():\n",
    "    bw, bh = int(x1 - x0), int(y1 - y0)\n",
    "    img_annotations.append({\n",
    "      \"image_id\": id2idx[img_id],\n",
    "      \"category_id\": LABEL2ID[label],\n",
    "      \"area\": int(bw * bh),\n",
    "      \"bbox\": [x0, y0, bw, bh]\n",
    "    })\n",
    "\n",
    "  annotated[img_id] = {\n",
    "    \"filepath\": f\"{IMG_DIR}/{img_dir}/{img_name}\",\n",
    "    \"image_id\": id2idx[img_id],\n",
    "    \"annotations\": img_annotations\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b04dcc",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84246dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrDataset(Dataset):\n",
    "  def __init__(self, ids, annotations, processor_name):\n",
    "    SIZE = { \"shortest_edge\": 640, \"longest_edge\": 1280 }\n",
    "    self.ids = ids\n",
    "    self.processor = AutoImageProcessor.from_pretrained(processor_name, size=SIZE)\n",
    "    self.annotations = annotations\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.ids)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    id = self.ids[idx]\n",
    "    annotations = self.annotations[id]\n",
    "    fpath = annotations[\"filepath\"]\n",
    "    img = PImage.open(fpath)\n",
    "\n",
    "    processed = self.processor(images=img, annotations=annotations, return_tensors=\"pt\")\n",
    "\n",
    "    return {\n",
    "      \"pixel_values\": processed[\"pixel_values\"].squeeze(),\n",
    "      \"labels\": processed[\"labels\"][0],\n",
    "    }\n",
    "\n",
    "  def collate_fn(self, batch):\n",
    "    pixel_values = t_stack([x[\"pixel_values\"] for x in batch])\n",
    "    padded = self.processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "\n",
    "    return {\n",
    "      \"pixel_values\": padded[\"pixel_values\"],\n",
    "      \"pixel_mask\": padded[\"pixel_mask\"],\n",
    "      \"labels\": [x[\"labels\"] for x in batch],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94de0b11",
   "metadata": {},
   "source": [
    "## PyTorchLightning Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetrTrain(LightningModule):\n",
    "  def __init__(self, model_name, label2id, lr, lr_backbone, weight_decay):\n",
    "    super().__init__()\n",
    "    self.model = DetrForObjectDetection.from_pretrained(\n",
    "      model_name,\n",
    "      revision=\"no_timm\",\n",
    "      label2id=label2id,\n",
    "      id2label={ id:label for label,id in label2id.items() },\n",
    "      ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    self.lr = lr\n",
    "    self.lr_backbone = lr_backbone\n",
    "    self.weight_decay = weight_decay\n",
    "\n",
    "    self.save_hyperparameters()\n",
    "\n",
    "  def forward(self, pixel_values, pixel_mask):\n",
    "    return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "  def common_step(self, batch, split):\n",
    "    outputs = self.model(pixel_values=batch[\"pixel_values\"],\n",
    "                         pixel_mask=batch[\"pixel_mask\"],\n",
    "                         labels=batch[\"labels\"])\n",
    "\n",
    "    self.log(f\"{split}_loss\", outputs.loss)\n",
    "    for k,v in outputs.loss_dict.items():\n",
    "      self.log(f\"{split}_{k}\", v.item())\n",
    "\n",
    "    return outputs.loss\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    loss = self.common_step(batch, \"train\")\n",
    "    return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    loss = self.common_step(batch, \"val\")\n",
    "    return loss\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    param_dicts = [\n",
    "      { \"params\": [p for n,p in self.named_parameters() if \"backbone\" not in n and p.requires_grad] },\n",
    "      {\n",
    "        \"params\": [p for n,p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "        \"lr\": self.lr_backbone,\n",
    "      },\n",
    "    ]\n",
    "    return AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f7edd3",
   "metadata": {},
   "source": [
    "## Instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd688aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "DETR_MODEL_NAME = \"facebook/detr-resnet-50\"\n",
    "\n",
    "train_ds = DetrDataset(list(tids), annotations=annotated, processor_name=DETR_MODEL_NAME)\n",
    "val_ds = DetrDataset(list(vids), annotations=annotated, processor_name=DETR_MODEL_NAME)\n",
    "\n",
    "train_dl = DataLoader(train_ds, collate_fn=train_ds.collate_fn, batch_size=8, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, collate_fn=val_ds.collate_fn, batch_size=16, shuffle=False)\n",
    "\n",
    "detr_train = DetrTrain(DETR_MODEL_NAME, LABEL2ID, lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d6f49",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2054456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch0 = next(iter(val_dl))\n",
    "out0 = detr_train.to(\"cuda\")(pixel_values=batch0[\"pixel_values\"].to(\"cuda\"), pixel_mask=batch0[\"pixel_mask\"].to(\"cuda\"))\n",
    "out0.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c244218",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e27ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(max_epochs=64, gradient_clip_val=0.1, fast_dev_run=False, log_every_n_steps=20)\n",
    "detr_train.train()\n",
    "trainer.fit(detr_train, train_dl, val_dl)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
